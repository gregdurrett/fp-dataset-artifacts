{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad8e9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435ff2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7279823c744feab7824a8d9877c262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8135fcf1c1f64246b2ec11b577acfd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7b3acc23414784a47263da70a47908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be70fcc44984caa91f8692fca342d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351759dba2904699b52fe6e50a7e1a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f536f7dcbf43b2930081ae7297df4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1a3be164af4ce2b76786bd9b7347b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fabef35d2f4c439ae72f4a320857c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /home/x/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b6141ed115415a91e5d53eec64e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52b09f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c269c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31cc7a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair as given in the references (see below)\n",
       "        - 'prediction_text': the text of the answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair (see above),\n",
       "        - 'answers': a Dict in the SQuAD dataset format\n",
       "            {\n",
       "                'text': list of possible texts for the answer, as a list of strings\n",
       "                'answer_start': list of start positions for the answer, as a list of ints\n",
       "            }\n",
       "            Note that answer_start values are not taken into account to compute the metric.\n",
       "Returns:\n",
       "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
       "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'exact_match': 100.0, 'f1': 100.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric('squad')\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c6783",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69cbf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2247856",
   "metadata": {},
   "source": [
    "## Naive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ae030d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dataset['train'][0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b4c3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".\n",
      "\n",
      "Q: What is human life expectancy in the United States?\n",
      "A: Human life expectancy in the United States is 78 years.\n",
      "\n",
      "Q: Who was president of the United States in 1955?\n",
      "A: Dwight D. Eisenhower was president of the United States in 1955.\n",
      "\n",
      "Q: Which party did he belong to?\n",
      "A: He belonged to the Republican Party.\n",
      "\n",
      "Q: What is the square root of banana?\n",
      "A: Unknown\n",
      "\n",
      "Q: How does a telescope work?\n",
      "A: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n",
      "\n",
      "Q: Where were the 1992 Olympics held?\n",
      "A: The 1992 Olympics were held in Barcelona, Spain.\n",
      "\n",
      "Q: How many squigs are in a bonk?\n",
      "A: Unknown\n",
      "\n",
      "Q: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".\n",
    "\n",
    "Q: What is human life expectancy in the United States?\n",
    "A: Human life expectancy in the United States is 78 years.\n",
    "\n",
    "Q: Who was president of the United States in 1955?\n",
    "A: Dwight D. Eisenhower was president of the United States in 1955.\n",
    "\n",
    "Q: Which party did he belong to?\n",
    "A: He belonged to the Republican Party.\n",
    "\n",
    "Q: What is the square root of banana?\n",
    "A: Unknown\n",
    "\n",
    "Q: How does a telescope work?\n",
    "A: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n",
    "\n",
    "Q: Where were the 1992 Olympics held?\n",
    "A: The 1992 Olympics were held in Barcelona, Spain.\n",
    "\n",
    "Q: How many squigs are in a bonk?\n",
    "A: Unknown\n",
    "\n",
    "Q: \"\"\" + x['question'] + \"\"\"\n",
    "A:\"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080ed6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-46Eaxhfzr7enI6OuOvgJ9CBgXhsrM at 0x7fb2aa311c20> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" Unknown\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1637391419,\n",
       "  \"id\": \"cmpl-46Eaxhfzr7enI6OuOvgJ9CBgXhsrM\",\n",
       "  \"model\": \"ada:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine=\"ada\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa631142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-46Eb9EBNbnkLHkgJxTl4tVBvWneQ9 at 0x7fb2a9f21e00> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" The Virgin Mary allegedly appeared to a French peasant named Marie Laveau in 1858 in Lourdes France.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1637391431,\n",
       "  \"id\": \"cmpl-46Eb9EBNbnkLHkgJxTl4tVBvWneQ9\",\n",
       "  \"model\": \"babbage:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine=\"babbage\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cc254a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-46EbJcQsp9lx3O6cFT9lVqBdvVmrE at 0x7fb2b0a69cc0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" The Virgin Mary allegedly appeared to a young girl named Bernadette Soubirous in 1858 in Lourdes France.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1637391441,\n",
       "  \"id\": \"cmpl-46EbJcQsp9lx3O6cFT9lVqBdvVmrE\",\n",
       "  \"model\": \"curie:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine=\"curie\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0974500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-46EbRY25gatNMDBX15ubJE4tfKROQ at 0x7fb2abe64450> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" The Virgin Mary allegedly appeared to Bernadette Soubirous in 1858 in Lourdes, France.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1637391449,\n",
       "  \"id\": \"cmpl-46EbRY25gatNMDBX15ubJE4tfKROQ\",\n",
       "  \"model\": \"davinci:2020-05-03\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "  engine=\"davinci\",\n",
    "  prompt=prompt,\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"\\n\"]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c88bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
