{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple notebook that calculates the model performance metrics\n",
    "The three metrcis calculated are:\n",
    "* Accuracy,\n",
    "* Jensen-Shannon divergence, and\n",
    "* Kullbackâ€“Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This comes from the ChaosNLI github repository:\n",
    "# https://github.com/easonnie/ChaosNLI/blob/master/distnli/src/evaluation/tools.py#L24\n",
    "def model_label_dist(logits_list):\n",
    "\n",
    "    logits = np.asarray(logits_list)\n",
    "    prob = np.exp(logits_list) / np.sum(np.exp(logits_list))\n",
    "\n",
    "    # numerical stability for KL\n",
    "    for i, value in enumerate(prob):\n",
    "        if np.abs(value) < 1e-15:\n",
    "            prob[i] = 1e-15\n",
    "    \n",
    "    # normalize\n",
    "    prob = prob / np.sum(prob)\n",
    "    assert np.isclose(np.sum(prob), 1)\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "def generate_metrics_dict(df):\n",
    "    \n",
    "    return {'acc': len(df.query('label_g == predicted_label'))/len(df),\n",
    "            'jsd': df.jsd.mean(),\n",
    "            'kld': df.kld.mean()}\n",
    "\n",
    "\n",
    "def generate_all_metrics_dict(df):\n",
    "\n",
    "    return {'all': generate_metrics_dict(eval_df),\n",
    "            'no-ambiguity': generate_metrics_dict(eval_df.query('amb_level == 0')),\n",
    "            'medium-ambiguity': generate_metrics_dict(eval_df.query('amb_level == 1')),\n",
    "            'high-ambiguity': generate_metrics_dict(eval_df.query('amb_level == 2'))}\n",
    "\n",
    "\n",
    "def calculate_example_metrics(df):\n",
    "\n",
    "    df['predicted_probs'] = df.apply(lambda x: model_label_dist(x.predicted_scores), axis=1)\n",
    "    df['jsd'] = df.apply(lambda x: jensenshannon(x.label, x.predicted_probs), axis=1)\n",
    "    df['kld'] = df.apply(lambda x: entropy(x.label, x.predicted_probs), axis=1)\n",
    "    df['amb_level'] = df.apply(lambda x: get_ambiguity_level(x), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_ambiguity_level(x):\n",
    "\n",
    "    # this whole thing is a bit klugy, but <shrug> it works since there might be some fp imprecision.\n",
    "    label_count = max(x.label)*5\n",
    "\n",
    "    if label_count > 4.5:\n",
    "        return 0\n",
    "    elif label_count > 3.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_OUTPUT_PATH = '/Users/richardross/workspace/msds/dsc-395t-nlp-final-project/fp-dataset-artifacts/output_final'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the baseline performance\n",
    "This model has been trained on the training dataset using gold labels, fine tuned on the dev dataset using gold labels, and evaluated on the test dataset using the probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'acc': 0.8890472312703583,\n",
       "  'jsd': 0.19018686160110934,\n",
       "  'kld': 0.42401248721189044},\n",
       " 'no-ambiguity': {'acc': 0.9544101228135468,\n",
       "  'jsd': 0.10453239072285646,\n",
       "  'kld': 0.135539159308617},\n",
       " 'medium-ambiguity': {'acc': 0.8847091605712295,\n",
       "  'jsd': 0.2620504238347566,\n",
       "  'kld': 0.6118892730848189},\n",
       " 'high-ambiguity': {'acc': 0.6744775174160861,\n",
       "  'jsd': 0.35103983140917144,\n",
       "  'kld': 1.0642033750592708}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.read_json(f'{BASE_OUTPUT_PATH}/trained_model_snli_baseline/trained_train_gold_dev_gold_evaled_test_probs/eval_predictions.jsonl', lines=True)\n",
    "eval_df = calculate_example_metrics(eval_df)\n",
    "generate_all_metrics_dict(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the baseline performance\n",
    "This model has been trained on the training dataset using gold labels and evaluated on the test dataset using the probability distributions (no fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'acc': 0.8897597719869706,\n",
       "  'jsd': 0.1961546240063928,\n",
       "  'kld': 0.3613578620539617},\n",
       " 'no-ambiguity': {'acc': 0.9531075548939337,\n",
       "  'jsd': 0.12868704955131746,\n",
       "  'kld': 0.1388770659038094},\n",
       " 'medium-ambiguity': {'acc': 0.8906304423545803,\n",
       "  'jsd': 0.24911370497669294,\n",
       "  'kld': 0.5053047694248999},\n",
       " 'high-ambiguity': {'acc': 0.6725775807473084,\n",
       "  'jsd': 0.32948282138184765,\n",
       "  'kld': 0.8568234905840153}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.read_json(f'{BASE_OUTPUT_PATH}/trained_model_snli_baseline/trained_train_gold_eval_test_probs/eval_predictions.jsonl', lines=True)\n",
    "eval_df = calculate_example_metrics(eval_df)\n",
    "generate_all_metrics_dict(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the probability distribution performance\n",
    "This model has been trained on the training dataset using gold labels, fine tuned on the dev dataset using probability distributions, and evaluated on the test dataset using the probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'acc': 0.8907776872964169,\n",
       "  'jsd': 0.21193841990962692,\n",
       "  'kld': 0.25899654742502876},\n",
       " 'no-ambiguity': {'acc': 0.9538518794194268,\n",
       "  'jsd': 0.18520251934684548,\n",
       "  'kld': 0.16976154564140516},\n",
       " 'medium-ambiguity': {'acc': 0.8857540926506444,\n",
       "  'jsd': 0.22325239907851072,\n",
       "  'kld': 0.30014324071045295},\n",
       " 'high-ambiguity': {'acc': 0.6852438252058265,\n",
       "  'jsd': 0.28236039294985604,\n",
       "  'kld': 0.48788618844006404}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.read_json(f'{BASE_OUTPUT_PATH}/trained_model_snli_baseline/trained_train_gold_dev_probs_evaled_test_probs/eval_predictions.jsonl', lines=True)\n",
    "eval_df = calculate_example_metrics(eval_df)\n",
    "generate_all_metrics_dict(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc-395t-nlp-final-project-new",
   "language": "python",
   "name": "dsc-395t-nlp-final-project-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
