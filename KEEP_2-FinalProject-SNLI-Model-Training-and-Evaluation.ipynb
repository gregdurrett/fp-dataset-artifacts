{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ecp5akkCyTz2"
   },
   "source": [
    "# Simple notebook for training and evaluating models\n",
    "\n",
    "Two types of models are trained, but both derive from a single initial Electra-small model trained on the SNLI train dataset. Following this, the two basic models are:\n",
    "* One fine-tuned on the SNLI dev dataset using gold labels and evaluated on the SNLI test dataset modified to contain a probility distribution of the annotator labels, and\n",
    "* One fine-tuned on the SNLI dev dataset modified to contain probility distributions of the annotator labels and evaluated on the SNLI test dataset modified to contain probility distributions of the annotator labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WkS00fT0xNHy"
   },
   "outputs": [],
   "source": [
    "ROOT_DATA_PATH = '../data/snli_1.0'\n",
    "ROOT_MODEL_PATH = './output_final/trained_model_snli_baseline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q2DoAMd-J2h"
   },
   "source": [
    "## Train the model using `train` dataset with gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luNUWEpu-Fuo",
    "outputId": "aca86bbc-469a-402c-cac3-88e09db75522"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-1477a4982449cb13\n",
      "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-1477a4982449cb13/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "\r",
      "  0% 0/1 [00:00<?, ?it/s]\r",
      "100% 1/1 [00:00<00:00, 209.44it/s]\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1477a4982449cb13/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-45b1a1d67ecbae2a.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1477a4982449cb13/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ba5be83f50120ab3.arrow\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 549367\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8241\n",
      "  0% 0/8241 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.6351, 'learning_rate': 4.69663875743235e-05, 'epoch': 0.18}\n",
      "  6% 500/8241 [05:12<1:20:15,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.483, 'learning_rate': 4.393277514864701e-05, 'epoch': 0.36}\n",
      " 12% 1000/8241 [10:25<1:15:11,  1.60it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.4511, 'learning_rate': 4.089916272297051e-05, 'epoch': 0.55}\n",
      " 18% 1500/8241 [15:37<1:10:03,  1.60it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 0.4266, 'learning_rate': 3.786555029729402e-05, 'epoch': 0.73}\n",
      " 24% 2000/8241 [20:50<1:04:38,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 0.4098, 'learning_rate': 3.4831937871617525e-05, 'epoch': 0.91}\n",
      " 30% 2500/8241 [26:03<59:35,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 0.3858, 'learning_rate': 3.179832544594103e-05, 'epoch': 1.09}\n",
      " 36% 3000/8241 [31:15<54:16,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 0.3696, 'learning_rate': 2.8764713020264533e-05, 'epoch': 1.27}\n",
      " 42% 3500/8241 [36:27<49:02,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 0.3657, 'learning_rate': 2.5731100594588037e-05, 'epoch': 1.46}\n",
      " 49% 4000/8241 [41:40<43:59,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 0.3633, 'learning_rate': 2.269748816891154e-05, 'epoch': 1.64}\n",
      " 55% 4500/8241 [46:52<38:33,  1.62it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 0.3625, 'learning_rate': 1.9663875743235045e-05, 'epoch': 1.82}\n",
      " 61% 5000/8241 [52:05<33:43,  1.60it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 0.3543, 'learning_rate': 1.6630263317558552e-05, 'epoch': 2.0}\n",
      " 67% 5500/8241 [57:16<28:17,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 0.3318, 'learning_rate': 1.3596650891882054e-05, 'epoch': 2.18}\n",
      " 73% 6000/8241 [1:02:28<23:11,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 0.3232, 'learning_rate': 1.0563038466205558e-05, 'epoch': 2.37}\n",
      " 79% 6500/8241 [1:07:40<18:03,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 0.3257, 'learning_rate': 7.529426040529063e-06, 'epoch': 2.55}\n",
      " 85% 7000/8241 [1:12:52<12:51,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.3298, 'learning_rate': 4.495813614852567e-06, 'epoch': 2.73}\n",
      " 91% 7500/8241 [1:18:04<07:41,  1.61it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7500\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7500/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.3234, 'learning_rate': 1.462201189176071e-06, 'epoch': 2.91}\n",
      " 97% 8000/8241 [1:23:17<02:30,  1.60it/s]Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-8000\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-8000/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/checkpoint-8000/special_tokens_map.json\n",
      "100% 8241/8241 [1:25:47<00:00,  1.65it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5147.782, 'train_samples_per_second': 320.157, 'train_steps_per_second': 1.601, 'train_loss': 0.3881980526607862, 'epoch': 3.0}\n",
      "100% 8241/8241 [1:25:47<00:00,  1.60it/s]\n",
      "Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!python run.py --do_train \\\n",
    "      --task nli \\\n",
    "      --per_device_train_batch_size 200 \\\n",
    "      --dataset ../data/snli_1.0/snli_1.0_train_prepared.jsonl \\\n",
    "      --output_dir $ROOT_MODEL_PATH/trained_train_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9vMWshgH_w5"
   },
   "source": [
    "## Train/fine-tune the model using `dev` dataset with gold labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MV6iNLuyPEdZ",
    "outputId": "c2bb2e30-a118-40e5-8e71-99b99a15d561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-3a4e6c19b1b5fa0b\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3a4e6c19b1b5fa0b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 2113.00it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 72.76it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3a4e6c19b1b5fa0b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 754.10it/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "#0:   0% 0/5 [00:00<?, ?ba/s]\n",
      "#1:   0% 0/5 [00:00<?, ?ba/s]\u001b[A\n",
      "#0:  20% 1/5 [00:00<00:01,  3.69ba/s]\n",
      "#0:  40% 2/5 [00:00<00:00,  4.50ba/s]\n",
      "#0:  60% 3/5 [00:00<00:00,  4.72ba/s]\n",
      "#1:  80% 4/5 [00:00<00:00,  4.02ba/s]\n",
      "#0:  80% 4/5 [00:01<00:00,  3.92ba/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9842\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "  0% 0/150 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 150/150 [01:33<00:00,  2.08it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 93.0311, 'train_samples_per_second': 317.378, 'train_steps_per_second': 1.612, 'train_loss': 0.2720386505126953, 'epoch': 3.0}\n",
      "100% 150/150 [01:33<00:00,  1.61it/s]\n",
      "Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold_dev_gold\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_gold/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_gold/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_gold/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_gold/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!python run.py --do_train \\\n",
    "      --task nli \\\n",
    "      --per_device_train_batch_size 200 \\\n",
    "      --dataset ../data/snli_1.0/snli_1.0_dev_prepared.jsonl \\\n",
    "      --output_dir $ROOT_MODEL_PATH/trained_train_gold_dev_gold \\\n",
    "      --model $ROOT_MODEL_PATH/trained_train_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdePb61sIIAJ"
   },
   "source": [
    "## Train/fine-tune the model using `dev` dataset with probabity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDdA2iOHPErN",
    "outputId": "9bba37c9-c9e9-42dd-8ad0-ae6d72f1ec46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-67abd46323a82e7a\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-67abd46323a82e7a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 2048.00it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 94.08it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-67abd46323a82e7a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 671.95it/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "#0:   0% 0/5 [00:00<?, ?ba/s]\n",
      "#0:  20% 1/5 [00:00<00:00,  4.47ba/s]\n",
      "#0:  40% 2/5 [00:00<00:00,  4.66ba/s]\n",
      "#0:  60% 3/5 [00:00<00:00,  4.69ba/s]\n",
      "#0:  80% 4/5 [00:00<00:00,  4.89ba/s]\n",
      "#0:  80% 4/5 [00:01<00:00,  3.93ba/s]\n",
      "#1:  80% 4/5 [00:01<00:00,  3.91ba/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9842\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n",
      "  0% 0/150 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 150/150 [01:32<00:00,  2.08it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 92.3497, 'train_samples_per_second': 319.72, 'train_steps_per_second': 1.624, 'train_loss': 0.5001451619466146, 'epoch': 3.0}\n",
      "100% 150/150 [01:32<00:00,  1.62it/s]\n",
      "Saving model checkpoint to ./output_final/trained_model_snli_baseline/trained_train_gold_dev_probs\n",
      "Configuration saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_probs/config.json\n",
      "Model weights saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_probs/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_probs/tokenizer_config.json\n",
      "Special tokens file saved in ./output_final/trained_model_snli_baseline/trained_train_gold_dev_probs/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "!python run.py --do_train \\\n",
    "      --task pnli \\\n",
    "      --per_device_train_batch_size 200 \\\n",
    "      --dataset ../data/snli_1.0/snli_1.0_dev_probs.jsonl \\\n",
    "      --output_dir $ROOT_MODEL_PATH/trained_train_gold_dev_probs \\\n",
    "      --model $ROOT_MODEL_PATH/trained_train_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqsrDGZvn7g5"
   },
   "source": [
    "## Evaluate the train gold, dev gold model using `test` dataset with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP2pqUsOn01W",
    "outputId": "67c07257-1950-4eb9-e837-6f892f668865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-daaa4f8822524b32\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-daaa4f8822524b32/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
      "Downloading data files: 100% 1/1 [00:00<00:00, 1943.61it/s]\n",
      "Extracting data files: 100% 1/1 [00:00<00:00, 107.83it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-daaa4f8822524b32/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
      "100% 1/1 [00:00<00:00, 687.48it/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "#0:   0% 0/5 [00:00<?, ?ba/s]\n",
      "#0:  20% 1/5 [00:00<00:00,  4.32ba/s]\n",
      "#0:  40% 2/5 [00:00<00:00,  4.33ba/s]\n",
      "#0:  60% 3/5 [00:00<00:00,  4.62ba/s]\n",
      "#0:  80% 4/5 [00:00<00:00,  4.83ba/s]\n",
      "#0:  80% 4/5 [00:01<00:00,  3.89ba/s]\n",
      "#1:  80% 4/5 [00:01<00:00,  3.82ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9824\n",
      "  Batch size = 8\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 1228/1228 [00:16<00:00, 76.16it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.6884621381759644, 'eval_accuracy': 0.8890472054481506, 'eval_runtime': 16.7748, 'eval_samples_per_second': 585.639, 'eval_steps_per_second': 73.205}\n"
     ]
    }
   ],
   "source": [
    "!python run.py --do_eval \\\n",
    "      --task pnli \\\n",
    "      --per_device_train_batch_size 200 \\\n",
    "      --dataset ../data/snli_1.0/snli_1.0_test_probs.jsonl \\\n",
    "      --output_dir $ROOT_MODEL_PATH/trained_train_gold_dev_gold_evaled_test_probs \\\n",
    "      --model $ROOT_MODEL_PATH/trained_train_gold_dev_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt_MfsLMo567"
   },
   "source": [
    "## Evaluate the train gold, dev probabilities model using `test` dataset with probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b9Zi8fEoyMv",
    "outputId": "8fe32bd3-545c-45ad-8aa9-23ba41983620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-daaa4f8822524b32\n",
      "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-daaa4f8822524b32/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "\r",
      "  0% 0/1 [00:00<?, ?it/s]\r",
      "100% 1/1 [00:00<00:00, 638.99it/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "#0:   0% 0/5 [00:00<?, ?ba/s]\n",
      "#1:   0% 0/5 [00:00<?, ?ba/s]\u001b[A\n",
      "#0:  20% 1/5 [00:00<00:01,  2.03ba/s]\n",
      "#0:  40% 2/5 [00:00<00:01,  2.51ba/s]\n",
      "#0:  60% 3/5 [00:01<00:00,  2.50ba/s]\n",
      "#1:  80% 4/5 [00:01<00:00,  2.17ba/s]\n",
      "#0:  80% 4/5 [00:01<00:00,  2.04ba/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9824\n",
      "  Batch size = 8\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 1228/1228 [00:15<00:00, 77.95it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.5234462022781372, 'eval_accuracy': 0.8907777070999146, 'eval_runtime': 16.4102, 'eval_samples_per_second': 598.652, 'eval_steps_per_second': 74.831}\n"
     ]
    }
   ],
   "source": [
    "!python run.py --do_eval \\\n",
    "      --task pnli \\\n",
    "      --per_device_train_batch_size 200 \\\n",
    "      --dataset ../data/snli_1.0/snli_1.0_test_probs.jsonl \\\n",
    "      --output_dir $ROOT_MODEL_PATH/trained_train_gold_dev_probs_evaled_test_probs \\\n",
    "      --model $ROOT_MODEL_PATH/trained_train_gold_dev_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDXyetcRpPnp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
